{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMb2Ap+w0R/mtffqyPI+Jc5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malcolmlett/ml-learning/blob/main/Gradient_understanding_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For a blog post on gradients within deep neural networks"
      ],
      "metadata": {
        "id": "6q5FH7m0SaD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvqlK-DuSVwu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extremely simple model\n",
        "# conclusions:\n",
        "#  - dY/dW is proportional to n\n",
        "#  - dJ/dW is averaged across all n if that's was the loss function does, however\n",
        "#  - dJ/dW will be summed across n, or any other operation, depending how the loss is computed.\n",
        "W = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "X = tf.ones(shape=(15,3))\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Y_pred = tf.matmul(X, W)\n",
        "  loss1 = tf.reduce_mean(Y_pred)\n",
        "  loss2 = tf.reduce_sum(Y_pred)\n",
        "\n",
        "print(f\"dY/dW: {tape.gradient(Y_pred, W)}\")\n",
        "print(f\"dJ1/dW: {tape.gradient(loss1, W)}\")\n",
        "print(f\"dJ2/dW: {tape.gradient(loss2, W)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_7c3cxjSh_F",
        "outputId": "499c2133-174f-426e-b521-4a15db0b04d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dY/dW: [[15. 15. 15.]\n",
            " [15. 15. 15.]\n",
            " [15. 15. 15.]]\n",
            "dJ1/dW: [[0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]]\n",
            "dJ2/dW: [[15. 15. 15.]\n",
            " [15. 15. 15.]\n",
            " [15. 15. 15.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-layer simple model without bias or activations\n",
        "W1 = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "W2 = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "W3 = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "X = tf.ones(shape=(5,3))\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Z1 = tf.matmul(X, W1)\n",
        "  Z2 = tf.matmul(Z1, W2)  # even if we had an activation function, none of these are any different than for the single layer case;\n",
        "  Z3 = tf.matmul(Z2, W3)  # so they still have to be SUMMED over n\n",
        "\n",
        "  Y_pred = Z3\n",
        "  loss = tf.reduce_mean(Y_pred)\n",
        "\n",
        "print(f\"dZ1/dW1: {tape.gradient(Z1, W1)}\")\n",
        "print(f\"dZ2/dW2: {tape.gradient(Z2, W2)}\")\n",
        "print(f\"dZ3/dW3: {tape.gradient(Z3, W3)}\")\n",
        "print(\"--\")\n",
        "print(f\"dZ1/dX:  {tape.gradient(Z1, X)}\")\n",
        "print(f\"dZ2/dA1: {tape.gradient(Z2, Z1)}\")\n",
        "print(f\"dZ3/dA2: {tape.gradient(Z3, Z2)}\")\n",
        "print(\"--\")\n",
        "print(f\"dY/dW1: {tape.gradient(Y_pred, W1)}\")\n",
        "print(f\"dY/dW2: {tape.gradient(Y_pred, W2)}\")\n",
        "print(f\"dY/dW3: {tape.gradient(Y_pred, W3)}\")\n",
        "print(\"--\")\n",
        "print(f\"dJ/dW1: {tape.gradient(loss, W1)}\")\n",
        "print(f\"dJ/dW2: {tape.gradient(loss, W2)}\")\n",
        "print(f\"dJ/dW3: {tape.gradient(loss, W3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C3R1dnfTt8t",
        "outputId": "28f58843-e90b-4652-d628-f8346e2de67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dZ1/dW1: [[5. 5. 5.]\n",
            " [5. 5. 5.]\n",
            " [5. 5. 5.]]\n",
            "dZ2/dW2: [[15. 15. 15.]\n",
            " [15. 15. 15.]\n",
            " [15. 15. 15.]]\n",
            "dZ3/dW3: [[45. 45. 45.]\n",
            " [45. 45. 45.]\n",
            " [45. 45. 45.]]\n",
            "--\n",
            "dZ1/dX:  None\n",
            "dZ2/dA1: [[3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]]\n",
            "dZ3/dA2: [[3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]]\n",
            "--\n",
            "dY/dW1: [[45. 45. 45.]\n",
            " [45. 45. 45.]\n",
            " [45. 45. 45.]]\n",
            "dY/dW2: [[45. 45. 45.]\n",
            " [45. 45. 45.]\n",
            " [45. 45. 45.]]\n",
            "dY/dW3: [[45. 45. 45.]\n",
            " [45. 45. 45.]\n",
            " [45. 45. 45.]]\n",
            "--\n",
            "dJ/dW1: [[3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]]\n",
            "dJ/dW2: [[3.0000002 3.0000002 3.0000002]\n",
            " [3.0000002 3.0000002 3.0000002]\n",
            " [3.0000002 3.0000002 3.0000002]]\n",
            "dJ/dW3: [[3.0000002 3.0000002 3.0000002]\n",
            " [3.0000002 3.0000002 3.0000002]\n",
            " [3.0000002 3.0000002 3.0000002]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-layer simple model with activation function modelled as a matrix multiplication\n",
        "#W1 = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "W1 = tf.Variable([[1, 0.5, 0.25],\n",
        "                  [0.75, 0.5, 0.3],\n",
        "                  [0.9, 0.3, 0.1]], dtype=tf.float32)\n",
        "S1 = tf.Variable(tf.linalg.diag([1., 1., 1.]), dtype=tf.float32)\n",
        "#S1 = tf.Variable(tf.linalg.diag([2., 0., 1.]), dtype=tf.float32)\n",
        "#W2 = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "W2 = tf.Variable([[1, 0.5, 0.25],\n",
        "                  [0.75, 0.5, 0.3],\n",
        "                  [0.9, 0.3, 0.1]], dtype=tf.float32)\n",
        "S2 = tf.Variable(tf.linalg.diag([1., 1., 1.]), dtype=tf.float32)\n",
        "W3 = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "S3 = tf.Variable(tf.linalg.diag([1., 1., 1.]), dtype=tf.float32)\n",
        "X = tf.ones(shape=(7,3))\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Z1 = tf.matmul(X, W1)\n",
        "  A1 = tf.matmul(Z1, S1)\n",
        "  Z2 = tf.matmul(A1, W2)\n",
        "  A2 = tf.matmul(Z2, S2)\n",
        "  Z3 = tf.matmul(A2, W3)\n",
        "  A3 = tf.matmul(Z3, S3)\n",
        "\n",
        "  Y_pred = A3\n",
        "  loss = tf.reduce_mean(Y_pred)\n",
        "\n",
        "print(f\"Z1: {Z1}\")\n",
        "print(f\"Z2: {Z2}\")\n",
        "print(f\"Z3: {Z3}\")\n",
        "print(\"--\")\n",
        "print(f\"A1: {A1}\")\n",
        "print(f\"A2: {A2}\")\n",
        "print(f\"A3: {A3}\")\n",
        "print(\"--\")\n",
        "print(f\"dJ/dA3: expect Y-Y_hat\\n{tape.gradient(loss, A3)}\")\n",
        "print(f\"dA3/dZ3: {tape.gradient(A3, Z3)}\") # expect S1.T         <-- nope, I'm getting ones() instead of diag()\n",
        "print(f\"dZ3/dA2: expect W3.T\\n{tape.gradient(Z3, A2)}\") # expect W3.T {tick}\n",
        "print(f\"dA2/dZ2: {tape.gradient(A2, Z2)}\") # expect S2.T         <-- nope, I'm getting ones() instead of diag()\n",
        "print(f\"dZ2/dA1: expect W2.T\\n{tape.gradient(Z2, A1)}\") # expect W2.T {tick}\n",
        "print(f\"dA1/dZ1: {tape.gradient(A1, Z1)}\") # expect S3.T         <-- nope, I'm getting ones() instead of diag()\n",
        "print(f\"dZ1/dX: expect W1.T\\n{tape.gradient(Z1, X)}\")  # expect W1.T {tick}\n",
        "print(\"--\")\n",
        "print(f\"dZ3/dW3: expect ~sum(n)<A2>\\n{tape.gradient(Z3, W3)}\") # expect sum A2 = [9]*5 = [45] {tick}, ie: [...]*n\n",
        "print(f\"dZ2/dW2: expect ~sum(n)<A1>\\n{tape.gradient(Z2, W2)}\") # expect sum A1 = [3]*5 = [15] {tick}, ie: [...]*n\n",
        "print(f\"dZ1/dW1: expect ~sum(n)<X>\\n{tape.gradient(Z1, W1)}\") # expect sum X  = [1]*5 = [5]  {tick},  ie: [...]*n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOY6weqSZljw",
        "outputId": "9eacfc6c-c08b-4b40-8997-670e9cb4dc9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z1: [[2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]]\n",
            "Z2: [[4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]]\n",
            "Z3: [[7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]]\n",
            "--\n",
            "A1: [[2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]\n",
            " [2.65       1.3        0.65000004]]\n",
            "A2: [[4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]\n",
            " [4.21      2.17      1.1175001]]\n",
            "A3: [[7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]\n",
            " [7.4975004 7.4975004 7.4975004]]\n",
            "--\n",
            "dJ/dA3: expect Y-Y_hat\n",
            "[[0.04761905 0.04761905 0.04761905]\n",
            " [0.04761905 0.04761905 0.04761905]\n",
            " [0.04761905 0.04761905 0.04761905]\n",
            " [0.04761905 0.04761905 0.04761905]\n",
            " [0.04761905 0.04761905 0.04761905]\n",
            " [0.04761905 0.04761905 0.04761905]\n",
            " [0.04761905 0.04761905 0.04761905]]\n",
            "dA3/dZ3: [[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "dZ3/dA2: expect W3.T\n",
            "[[3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]]\n",
            "dA2/dZ2: [[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "dZ2/dA1: expect W2.T\n",
            "[[1.75      1.55      1.3000001]\n",
            " [1.75      1.55      1.3000001]\n",
            " [1.75      1.55      1.3000001]\n",
            " [1.75      1.55      1.3000001]\n",
            " [1.75      1.55      1.3000001]\n",
            " [1.75      1.55      1.3000001]\n",
            " [1.75      1.55      1.3000001]]\n",
            "dA1/dZ1: [[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "dZ1/dX: expect W1.T\n",
            "None\n",
            "--\n",
            "dZ3/dW3: expect ~sum(n)<A2>\n",
            "[[29.470001 29.470001 29.470001]\n",
            " [15.190001 15.190001 15.190001]\n",
            " [ 7.8225    7.8225    7.8225  ]]\n",
            "dZ2/dW2: expect ~sum(n)<A1>\n",
            "[[18.550001 18.550001 18.550001]\n",
            " [ 9.099999  9.099999  9.099999]\n",
            " [ 4.55      4.55      4.55    ]]\n",
            "dZ1/dW1: expect ~sum(n)<X>\n",
            "[[7. 7. 7.]\n",
            " [7. 7. 7.]\n",
            " [7. 7. 7.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simulating an actual training loop, with a\n",
        "# multi-layer simple model with activation function modelled as a matrix multiplication\n",
        "#W1 = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "W1 = tf.Variable([[1, 0.5, 0.25],\n",
        "                  [0.75, 0.5, 0.3],\n",
        "                  [0.9, 0.3, 0.1]], dtype=tf.float32)\n",
        "S1 = tf.Variable(tf.linalg.diag([1., 1., 1.]), dtype=tf.float32)\n",
        "#S1 = tf.Variable(tf.linalg.diag([2., 0., 1.]), dtype=tf.float32)\n",
        "#W2 = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "W2 = tf.Variable([[1, 0.5, 0.25],\n",
        "                  [0.75, 0.5, 0.3],\n",
        "                  [0.9, 0.3, 0.1]], dtype=tf.float32)\n",
        "S2 = tf.Variable(tf.linalg.diag([1., 1., 1.]), dtype=tf.float32)\n",
        "#W3 = tf.Variable(tf.ones(shape=(3,3)), dtype=tf.float32)\n",
        "W3 = tf.Variable([[1, 0.5, 0.25],\n",
        "                  [0.75, 0.5, 0.3],\n",
        "                  [0.9, 0.3, 0.1]], dtype=tf.float32)\n",
        "S3 = tf.Variable(tf.linalg.diag([1., 1., 1.]), dtype=tf.float32)\n",
        "\n",
        "X = tf.ones(shape=(7,3))\n",
        "Y = tf.ones(shape=(7,1))\n",
        "n = X.shape[0]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  Z1 = tf.matmul(X, W1)\n",
        "  A1 = tf.matmul(Z1, S1)\n",
        "  Z2 = tf.matmul(A1, W2)\n",
        "  A2 = tf.matmul(Z2, S2)\n",
        "  Z3 = tf.matmul(A2, W3)\n",
        "  A3 = tf.where(Z3 > 0, Z3, np.zeros_like(Z3)) #tf.matmul(Z3, S3)\n",
        "\n",
        "  Y_pred = A3\n",
        "  loss = 1/(2*n) * tf.reduce_sum((Y - Y_pred)**2)  # MSE loss\n",
        "  loss2 = tf.keras.losses.MSE(Y, Y_pred)     ### BUG FIX: probably need to use tf.keras.losses.MSE()(Y, Y_pred) instead\n",
        "\n",
        "\n",
        "print(f\"n: {n}\")\n",
        "print(f\"loss: {loss}, {loss2}\")\n",
        "print(\"--\")\n",
        "print(f\"Y:\\n{Y}\")\n",
        "print(f\"Y_hat:\\n{Y_pred}\")\n",
        "print(f\"Error:\\n{Y - Y_pred}\")\n",
        "print(\"--\")\n",
        "print(f\"dJ/dA3: expect Y-Y_hat\\n{tape.gradient(loss, A3)}\")\n",
        "print(f\"dJ/dZ3: expect Y-Y_hat\\n{tape.gradient(loss, Z3)}\")\n",
        "print(f\"dJ2/dA3: expect Y-Y_hat\\n{tape.gradient(loss2, A3)}\")\n",
        "print(f\"dJ2/dZ3: expect Y-Y_hat\\n{tape.gradient(loss2, Z3)}\")\n",
        "print(\"--\")\n",
        "expect_W3 = (1/n) * tf.matmul(tf.transpose(A2), Y_pred - Y)  # note: gives wrong or transposed results in any other form\n",
        "print(f\"dJ/dW3: expected:\\n{expect_W3}\")\n",
        "print(f\"dJ/dW3: expect mean (error*x)\\n{tape.gradient(loss, W3)}\")\n",
        "print(f\"dJ2/dW3: expect mean (error*x)\\n{tape.gradient(loss2, W3)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1o92ufPf1RP",
        "outputId": "e8406c41-2574-4bf4-b213-356dc0666233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n: 7\n",
            "loss: 20.592548370361328, [13.728364 13.728364 13.728364 13.728364 13.728364 13.728364 13.728364]\n",
            "--\n",
            "Y:\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "Y_hat:\n",
            "[[6.8432503 3.5252502 1.81525  ]\n",
            " [6.8432503 3.5252502 1.81525  ]\n",
            " [6.8432503 3.5252502 1.81525  ]\n",
            " [6.8432503 3.5252502 1.81525  ]\n",
            " [6.8432503 3.5252502 1.81525  ]\n",
            " [6.8432503 3.5252502 1.81525  ]\n",
            " [6.8432503 3.52525   1.81525  ]]\n",
            "Error:\n",
            "[[-5.8432503  -2.5252502  -0.81525004]\n",
            " [-5.8432503  -2.5252502  -0.81525004]\n",
            " [-5.8432503  -2.5252502  -0.81525004]\n",
            " [-5.8432503  -2.5252502  -0.81525004]\n",
            " [-5.8432503  -2.5252502  -0.81525004]\n",
            " [-5.8432503  -2.5252502  -0.81525004]\n",
            " [-5.8432503  -2.52525    -0.81525004]]\n",
            "--\n",
            "dJ/dA3: expect Y-Y_hat\n",
            "[[0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075002 0.11646429]]\n",
            "dJ/dZ3: expect Y-Y_hat\n",
            "[[0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075005 0.11646429]\n",
            " [0.83475006 0.36075002 0.11646429]]\n",
            "dJ2/dA3: expect Y-Y_hat\n",
            "[[3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835     0.54350007]]\n",
            "dJ2/dZ3: expect Y-Y_hat\n",
            "[[3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835002  0.54350007]\n",
            " [3.8955002  1.6835     0.54350007]]\n",
            "--\n",
            "dJ/dW3: expected:\n",
            "[[-24.600086  -10.631305   -3.4322028]\n",
            " [-12.679854   -5.479793   -1.7690928]\n",
            " [ -6.5298324  -2.8219674  -0.911042 ]]\n",
            "dJ/dW3: expect mean (error*x)\n",
            "[[24.600084  10.631304   3.4322028]\n",
            " [12.679854   5.479793   1.7690927]\n",
            " [ 6.529833   2.8219674  0.911042 ]]\n",
            "dJ2/dW3: expect mean (error*x)\n",
            "[[114.80039    49.61275    16.016947 ]\n",
            " [ 59.17265    25.572367    8.255766 ]\n",
            " [ 30.472553   13.169181    4.2515297]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(-5.8432503)**2 + (-2.5252502)**2  + (-0.81525004)**2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOzNXwBiejCg",
        "outputId": "99329bab-2ca1-4caa-df0c-3b1b005fcc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41.185095268770134"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.square(Y - Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUZ7jDdDhFv2",
        "outputId": "b8570227-ad24-466a-980f-386c1f68ddbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7, 3), dtype=float32, numpy=\n",
              "array([[34.143574 ,  6.3768888,  0.6646326],\n",
              "       [34.143574 ,  6.3768888,  0.6646326],\n",
              "       [34.143574 ,  6.3768888,  0.6646326],\n",
              "       [34.143574 ,  6.3768888,  0.6646326],\n",
              "       [34.143574 ,  6.3768888,  0.6646326],\n",
              "       [34.143574 ,  6.3768888,  0.6646326],\n",
              "       [34.143574 ,  6.3768873,  0.6646326]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reduce_mean(tf.square(Y - Y_pred), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SFQ8YnIhg8t",
        "outputId": "ec309584-f888-4371-c519-435f527603d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7,), dtype=float32, numpy=\n",
              "array([13.728364, 13.728364, 13.728364, 13.728364, 13.728364, 13.728364,\n",
              "       13.728364], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reduce_sum(tf.square(Y - Y_pred), axis=1)/7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-H9ExZ-iUGF",
        "outputId": "91153856-9f93-4e3d-8ced-ffad0b4d014a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7,), dtype=float32, numpy=\n",
              "array([5.8835845, 5.8835845, 5.8835845, 5.8835845, 5.8835845, 5.8835845,\n",
              "       5.8835845], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reduce_sum(tf.square(Y - Y_pred), axis=1)/3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVwpgXTsibqs",
        "outputId": "832b9e44-da87-4f19-a962-639ad7c731b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7,), dtype=float32, numpy=\n",
              "array([13.728364, 13.728364, 13.728364, 13.728364, 13.728364, 13.728364,\n",
              "       13.728364], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MZrljXmifUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}